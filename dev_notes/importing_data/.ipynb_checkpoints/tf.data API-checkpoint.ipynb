{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed_dict system의 문제점\n",
    "\n",
    "우리는 이번 tutorial을 통해서 Data pipelining을 효율적으로 만드는 것에 중점적으로 확인하려고 한다.\n",
    "\n",
    "TensorFlow를 통해서 학습을 위한 코드를 작성할 때 크게 세 부분으로 나뉘는데,\n",
    "\n",
    "첫 째 input data를 로드하는 부분, \n",
    "\n",
    "둘 째 model을 디자인하고 input data를 받아 prediction을 출력하는 부분, \n",
    "\n",
    "셋 째 prediction을 정답과 비교하고 model의 파라미터를 갱신하는 부분으로 나눌 수 있다.\n",
    "\n",
    "우리는 코드를 통해 학습 연산을 구현할 때 우리의 연산 자원인 GPU의 효율성을 최대로 발휘하도록 해야 한다. 이때 많은 경우에 걸쳐 Bottle-neck으로 작용하는 것이 바로 첫 번째 input loading part이다. GPU를 효율적으로 사용하여 학습 속도를 가속하기 위해서는 지속적으로 쉬지 않고 data를 전달해주어야 한다. 만약 program이 data를 가져와서 model에 전달하는 부분과 data를 통해 연산하는 부분이 순차적으로 수행되게 된다면 GPU는 data를 가져와 넣어줄 때까지 놀게 될 것이다.\n",
    "\n",
    "TensorFlow의 feed_dict는 이점에서 문제가 있다. feed_dict는 python data를 session에게 복사하여 넘겨준다. 만약 single threading을 하는 program이라면 GPU는 data를 대기하며 idle이 발생할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-3aff98e80dad>:10: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "iter:0 - loss:2.912185\n",
      "iter:500 - loss:0.369858\n",
      "iter:1000 - loss:0.851633\n",
      "iter:1500 - loss:1.058526\n",
      "iter:2000 - loss:0.988433\n",
      "iter:2500 - loss:0.315225\n",
      "iter:3000 - loss:0.246361\n",
      "iter:3500 - loss:0.849347\n",
      "iter:4000 - loss:0.897075\n",
      "iter:4500 - loss:2.264365\n",
      "Time taken: 7.456434\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "# We simulate some raw input data \n",
    "# (think about it as fetching some data from the file system)\n",
    "# let's say: batches of 128 samples, each containing 1024 data points\n",
    "x_inputs_data = tf.random_normal([128, 1024], mean=0, stddev=1)\n",
    "# We will try to predict this law:\n",
    "# predict 1 if the sum of the elements is positive and 0 otherwise\n",
    "y_inputs_data = tf.cast(tf.reduce_sum(x_inputs_data, axis=1, keep_dims=True) > 0, tf.int32)\n",
    "\n",
    "# We build our small model: a basic two layers neural net with ReLU\n",
    "with tf.variable_scope(\"placeholder\"):\n",
    "    input = tf.placeholder(tf.float32, shape=[None, 1024])\n",
    "    y_true = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "with tf.variable_scope('FullyConnected'):\n",
    "    w = tf.get_variable('w', shape=[1024, 1024], initializer=tf.random_normal_initializer(stddev=1e-1))\n",
    "    b = tf.get_variable('b', shape=[1024], initializer=tf.constant_initializer(0.1))\n",
    "    z = tf.matmul(input, w) + b\n",
    "    y = tf.nn.relu(z)\n",
    "\n",
    "    w2 = tf.get_variable('w2', shape=[1024, 1], initializer=tf.random_normal_initializer(stddev=1e-1))\n",
    "    b2 = tf.get_variable('b2', shape=[1], initializer=tf.constant_initializer(0.1))\n",
    "    z = tf.matmul(y, w2) + b2\n",
    "with tf.variable_scope('Loss'):\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(None, tf.cast(y_true, tf.float32), z)\n",
    "    loss_op = tf.reduce_mean(losses)\n",
    "with tf.variable_scope('Accuracy'):\n",
    "    y_pred = tf.cast(z > 0, tf.int32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y_true), tf.float32))\n",
    "    accuracy = tf.Print(accuracy, data=[accuracy], message=\"accuracy:\")\n",
    "\n",
    "# We add the training operation, ...\n",
    "adam = tf.train.AdamOptimizer(1e-2)\n",
    "train_op = adam.minimize(loss_op, name=\"train_op\")\n",
    "\n",
    "startTime = time.time()\n",
    "with tf.Session() as sess:\n",
    "    # ... init our variables, ...\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # ... check the accuracy before training, ...\n",
    "    x_input, y_input = sess.run([x_inputs_data, y_inputs_data])\n",
    "    sess.run(accuracy, feed_dict={\n",
    "        input: x_input,\n",
    "        y_true: y_input\n",
    "    })\n",
    "\n",
    "    # ... train ...\n",
    "    for i in range(5000):\n",
    "        #  ... by sampling some input data (fetching) ...\n",
    "        x_input, y_input = sess.run([x_inputs_data, y_inputs_data])\n",
    "        # ... and feeding it to our model\n",
    "        _, loss = sess.run([train_op, loss_op], feed_dict={\n",
    "            input: x_input,\n",
    "            y_true: y_input\n",
    "        })\n",
    "\n",
    "        # We regularly check the loss\n",
    "        if i % 500 == 0:\n",
    "            print('iter:%d - loss:%f' % (i, loss))\n",
    "\n",
    "    # Finally, we check our final accuracy\n",
    "    x_input, y_input = sess.run([x_inputs_data, y_inputs_data])\n",
    "    sess.run(accuracy, feed_dict={\n",
    "        input: x_input,\n",
    "        y_true: y_input\n",
    "    })\n",
    "\n",
    "print(\"Time taken: %f\" % (time.time() - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "위에서 언급한 바대로 '데이터 입력' 부분과 '연산' 부분을 비동기화하여 GPU가 쉬지 않고 일할 수 잇다면, 훨씬 효율적으로 연산 자원을 활용할 수 있을 것이다. 우리는 multi-threading을 사용하여 데이터 입력 부분을 효율적으로 만들어줄 수 있다. TensorFlow에서는 이를 위해 Queue와 Queue runner를 API로 제공하고 있다.\n",
    "\n",
    "reference : https://blog.metaflow.fr/tensorflow-how-to-optimise-your-input-pipeline-with-queues-and-multi-threading-e7c3874157e0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queue_runner 사용 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# This time, let's start with 6 samples of 1 data point\n",
    "x_input_data = tf.random_normal([6], mean=-1, stddev=4)\n",
    "\n",
    "# Note that the FIFO queue has still a capacity of 3\n",
    "q = tf.FIFOQueue(capacity=3, dtypes=tf.float32)\n",
    "\n",
    "# To check what is happening in this case:\n",
    "# we will print a message each time \"x_input_data\" is actually computed\n",
    "# to be used in the \"enqueue_many\" operation\n",
    "x_input_data = tf.Print(x_input_data, data=[x_input_data], message=\"Raw inputs data generated:\", summarize=6)\n",
    "enqueue_op = q.enqueue_many(x_input_data)\n",
    "\n",
    "# To leverage multi-threading we create a \"QueueRunner\"\n",
    "# that will handle the \"enqueue_op\" outside of the main thread\n",
    "# We don't need much parallelism here, so we will use only 1 thread\n",
    "numberOfThreads = 1 \n",
    "qr = tf.train.QueueRunner(q, [enqueue_op] * numberOfThreads)\n",
    "# Don't forget to add your \"QueueRunner\" to the QUEUE_RUNNERS collection\n",
    "tf.train.add_queue_runner(qr) \n",
    "\n",
    "input = q.dequeue() \n",
    "input = tf.Print(input, data=[q.size(), input], message=\"Nb elements left, input:\")\n",
    "\n",
    "# fake graph: START\n",
    "y = input + 1\n",
    "# fake graph: END \n",
    "\n",
    "# We start the session as usual ...\n",
    "with tf.Session() as sess:\n",
    "    # But now we build our coordinator to coordinate our child threads with\n",
    "    # the main thread\n",
    "    coord = tf.train.Coordinator()\n",
    "    # Beware, if you don't start all your queues before runnig anything\n",
    "    # The main threads will wait for them to start and you will hang again\n",
    "    # This helper start all queues in tf.GraphKeys.QUEUE_RUNNERS\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    # The QueueRunner will automatically call the enqueue operation\n",
    "    # asynchronously in its own thread ensuring that the queue is always full\n",
    "    # No more hanging for the main process, no more waiting for the GPU\n",
    "    sess.run(y)\n",
    "    sess.run(y) \n",
    "    sess.run(y)\n",
    "    sess.run(y)\n",
    "    sess.run(y)\n",
    "    sess.run(y)\n",
    "    sess.run(y)\n",
    "    sess.run(y)\n",
    "    sess.run(y)\n",
    "    sess.run(y)\n",
    "\n",
    "    # We request our child threads to stop ...\n",
    "    coord.request_stop()\n",
    "    # ... and we wait for them to do so before releasing the main thread\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개선된 문제점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0 - loss:2.171846\n",
      "iter:500 - loss:0.469907\n",
      "iter:1000 - loss:0.633990\n",
      "iter:1500 - loss:1.030779\n",
      "iter:2000 - loss:0.545669\n",
      "iter:2500 - loss:0.323410\n",
      "iter:3000 - loss:0.787239\n",
      "iter:3500 - loss:0.652673\n",
      "iter:4000 - loss:0.695005\n",
      "iter:4500 - loss:1.328690\n",
      "Time taken: 4.120565\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "# We simulate some raw input data \n",
    "# (think about it as fetching some data from the file system)\n",
    "# let's say: batches of 128 samples, each containing 1024 data points\n",
    "x_input_data = tf.random_normal([128, 1024], mean=0, stddev=1)\n",
    "\n",
    "# We build our small model: a basic two layers neural net with ReLU\n",
    "\n",
    "with tf.variable_scope(\"While_Queue_runner\"):\n",
    "\n",
    "    with tf.variable_scope(\"queue\"):\n",
    "        q = tf.FIFOQueue(capacity=5, dtypes=tf.float32) # enqueue 5 batches\n",
    "        # We use the \"enqueue\" operation so 1 element of the queue is the full batch\n",
    "        enqueue_op = q.enqueue(x_input_data)\n",
    "        numberOfThreads = 1\n",
    "        qr = tf.train.QueueRunner(q, [enqueue_op] * numberOfThreads)\n",
    "        tf.train.add_queue_runner(qr)\n",
    "        input = q.dequeue() # It replaces our input placeholder\n",
    "        # We can also compute y_true right into the graph now\n",
    "        y_true = tf.cast(tf.reduce_sum(input, axis=1, keep_dims=True) > 0, tf.int32)\n",
    "\n",
    "    with tf.variable_scope('FullyConnected'):\n",
    "        w = tf.get_variable('w', shape=[1024, 1024], initializer=tf.random_normal_initializer(stddev=1e-1))\n",
    "        b = tf.get_variable('b', shape=[1024], initializer=tf.constant_initializer(0.1))\n",
    "        z = tf.matmul(input, w) + b\n",
    "        y = tf.nn.relu(z)\n",
    "\n",
    "        w2 = tf.get_variable('w2', shape=[1024, 1], initializer=tf.random_normal_initializer(stddev=1e-1))\n",
    "        b2 = tf.get_variable('b2', shape=[1], initializer=tf.constant_initializer(0.1))\n",
    "        z = tf.matmul(y, w2) + b2\n",
    "\n",
    "    with tf.variable_scope('Loss'):\n",
    "        losses = tf.nn.sigmoid_cross_entropy_with_logits(None, tf.cast(y_true, tf.float32), z)\n",
    "        loss_op = tf.reduce_mean(losses)\n",
    "\n",
    "    with tf.variable_scope('Accuracy'):\n",
    "        y_pred = tf.cast(z > 0, tf.int32)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y_true), tf.float32))\n",
    "        accuracy = tf.Print(accuracy, data=[accuracy], message=\"accuracy:\")\n",
    "\n",
    "    # We add the training op ...\n",
    "    adam = tf.train.AdamOptimizer(1e-2)\n",
    "    train_op = adam.minimize(loss_op, name=\"train_op\")\n",
    "\n",
    "startTime = time.time()\n",
    "with tf.Session() as sess:\n",
    "    # ... init our variables, ...\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # ... add the coordinator, ...\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    # ... check the accuracy before training (without feed_dict!), ...\n",
    "    sess.run(accuracy)\n",
    "\n",
    "    # ... train ...\n",
    "    for i in range(5000):\n",
    "        #  ... without sampling from Python and without a feed_dict !\n",
    "        _, loss = sess.run([train_op, loss_op])\n",
    "\n",
    "        # We regularly check the loss\n",
    "        if i % 500 == 0:\n",
    "            print('iter:%d - loss:%f' % (i, loss))\n",
    "\n",
    "    # Finally, we check our final accuracy\n",
    "    sess.run(accuracy)\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "\n",
    "print(\"Time taken: %f\" % (time.time() - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reference\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "\n",
    "https://www.tensorflow.org/guide/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing data with Tensorflow tf.data API\n",
    "\n",
    "tf.data는 간단한 수준에서 복잡한 수준까지의 Input pipeline을 구성할 수 있도록 하는 API이다. 구체적으로 제공하는 기능은 다음과 같다.\n",
    "\n",
    "# 1. tf.data.Dataset\n",
    "\n",
    "tf.data.Dataset은 연속된 element 집합으로, 각각의 element가 Tensor object로 구성되어있다. 이때 각각의 element는 Training을 위한 data와 label의 pair로 볼 수 있을 것이다.\n",
    "\n",
    "크게 다음과 같은 구성을 가진다.\n",
    "\n",
    "- Creating source : 다수의 tf.Tensor object 또는 file로부터 dataset을 구성한다.\n",
    "\n",
    "  e.g : Dataset.from_tensor_slices()\n",
    "  \n",
    "  그 외에도 from_generator, list_files, interleave\n",
    "\n",
    "\n",
    "- Applying a transformation : 하나 또는 여러 개의 dataset object들로부터 새로운 dataset을 구성한다.\n",
    "\n",
    "  e.g : Dataset.batch()\n",
    "  \n",
    "  그 외에도 concatenate, filter, reduce, map, flat_map, padded_batch, shard, zip, shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of data source\n",
    "1. tf.data.Dataset.from_tensors() or tf.data.Dataset.from_tensor_slices() :\n",
    "   memory 상의 tensor들로 dataset을 만드는 경우\n",
    "2. tf.data.TFRecordDataset :\n",
    "   Disk 상의 file들로 dataset을 만드는 경우\n",
    "3. tf.data.from_generator  :\n",
    "   python iterator로부터 dataset을 만드는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset and Show information\n",
    "output_shapes : dataset의 각 element의 shape 정보\n",
    "output_types : dataset의 각 element의 type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset1 : \n",
      "<dtype: 'float32'>\n",
      "(10,)\n",
      "dataset2 : \n",
      "(tf.float32, tf.int32)\n",
      "(TensorShape([Dimension(1)]), TensorShape([Dimension(10)]))\n",
      "dataset3 : \n",
      "(tf.float32, (tf.float32, tf.int32))\n",
      "(TensorShape([Dimension(10)]), (TensorShape([Dimension(1)]), TensorShape([Dimension(10)])))\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices( tf.random_uniform([4, 10], dtype = tf.float32) )\n",
    "print(\"dataset1 : \")\n",
    "print(dataset1.output_types)\n",
    "print(dataset1.output_shapes)\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices( \n",
    "    (tf.random_uniform( [4, 1] ),\n",
    "     tf.random_uniform( [4, 10], maxval=100, dtype = tf.int32)))\n",
    "\n",
    "print(\"dataset2 : \")\n",
    "print(dataset2.output_types)\n",
    "print(dataset2.output_shapes)\n",
    "    \n",
    "dataset3 = tf.data.Dataset.zip( (dataset1, dataset2) )\n",
    "print(\"dataset3 : \")\n",
    "print(dataset3.output_types)\n",
    "print(dataset3.output_shapes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-28-95f8998e7bfa>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-95f8998e7bfa>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    with tf.Session() as sess:\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "file_list_dataset = tf.data.Dataset.list_files(\n",
    "    '/home/dan/datasets/flower_photos/daisy/*.jpg',\n",
    "    shuffle=None,\n",
    "    seed=None\n",
    ")\n",
    "iterator = file_list_dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (10,), types: tf.float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 데이터를 표현하는 데이터셋을 만든 뒤에, 데이터셋의 element에 접근하는 iterator를 만들어야 한다. tf.data API는 다음과 같은 iterator를 제공하고 있다. \n",
    "\n",
    "- one-shot,\n",
    "- initializable,\n",
    "- reinitializable, and\n",
    "- feedable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 one_shot_iterator\n",
    "\n",
    "one shot iterator는 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(10):\n",
    "      value = sess.run(next_element)\n",
    "      print(value)\n",
    "      assert i == value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 다양한 source로부터의 입력\n",
    "   1. local file system\n",
    "   2. distributed file system\n",
    "   3. On-memory data\n",
    "   4. real-time data generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
