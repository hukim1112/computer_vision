{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed_dict system의 문제점\n",
    "\n",
    "우리는 이번 tutorial을 통해서 Data pipelining을 효율적으로 만드는 것에 중점적으로 확인하려고 한다.\n",
    "\n",
    "TensorFlow를 통해서 학습을 위한 코드를 작성할 때 크게 세 부분으로 나뉘는데,\n",
    "\n",
    "첫 째 input data를 로드하는 부분, \n",
    "\n",
    "둘 째 model을 디자인하고 input data를 받아 prediction을 출력하는 부분, \n",
    "\n",
    "셋 째 prediction을 정답과 비교하고 model의 파라미터를 갱신하는 부분으로 나눌 수 있다.\n",
    "\n",
    "우리는 코드를 통해 학습 연산을 구현할 때 우리의 연산 자원인 GPU의 효율성을 최대로 발휘하도록 해야 한다. 이때 많은 경우에 걸쳐 Bottle-neck으로 작용하는 것이 바로 첫 번째 input loading part이다. GPU를 효율적으로 사용하여 학습 속도를 가속하기 위해서는 지속적으로 쉬지 않고 data를 전달해주어야 한다. 만약 program이 data를 가져와서 model에 전달하는 부분과 data를 통해 연산하는 부분이 순차적으로 수행되게 된다면 GPU는 data를 가져와 넣어줄 때까지 놀게 될 것이다.\n",
    "\n",
    "TensorFlow의 feed_dict는 이점에서 문제가 있다. feed_dict는 python data를 session에게 복사하여 넘겨준다. 만약 single threading을 하는 program이라면 GPU는 data를 대기하며 idle이 발생할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-3aff98e80dad>:10: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "iter:0 - loss:2.912185\n",
      "iter:500 - loss:0.369858\n",
      "iter:1000 - loss:0.851633\n",
      "iter:1500 - loss:1.058526\n",
      "iter:2000 - loss:0.988433\n",
      "iter:2500 - loss:0.315225\n",
      "iter:3000 - loss:0.246361\n",
      "iter:3500 - loss:0.849347\n",
      "iter:4000 - loss:0.897075\n",
      "iter:4500 - loss:2.264365\n",
      "Time taken: 7.456434\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "# We simulate some raw input data \n",
    "# (think about it as fetching some data from the file system)\n",
    "# let's say: batches of 128 samples, each containing 1024 data points\n",
    "x_inputs_data = tf.random_normal([128, 1024], mean=0, stddev=1)\n",
    "# We will try to predict this law:\n",
    "# predict 1 if the sum of the elements is positive and 0 otherwise\n",
    "y_inputs_data = tf.cast(tf.reduce_sum(x_inputs_data, axis=1, keep_dims=True) > 0, tf.int32)\n",
    "\n",
    "# We build our small model: a basic two layers neural net with ReLU\n",
    "with tf.variable_scope(\"placeholder\"):\n",
    "    input = tf.placeholder(tf.float32, shape=[None, 1024])\n",
    "    y_true = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "with tf.variable_scope('FullyConnected'):\n",
    "    w = tf.get_variable('w', shape=[1024, 1024], initializer=tf.random_normal_initializer(stddev=1e-1))\n",
    "    b = tf.get_variable('b', shape=[1024], initializer=tf.constant_initializer(0.1))\n",
    "    z = tf.matmul(input, w) + b\n",
    "    y = tf.nn.relu(z)\n",
    "\n",
    "    w2 = tf.get_variable('w2', shape=[1024, 1], initializer=tf.random_normal_initializer(stddev=1e-1))\n",
    "    b2 = tf.get_variable('b2', shape=[1], initializer=tf.constant_initializer(0.1))\n",
    "    z = tf.matmul(y, w2) + b2\n",
    "with tf.variable_scope('Loss'):\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(None, tf.cast(y_true, tf.float32), z)\n",
    "    loss_op = tf.reduce_mean(losses)\n",
    "with tf.variable_scope('Accuracy'):\n",
    "    y_pred = tf.cast(z > 0, tf.int32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y_true), tf.float32))\n",
    "    accuracy = tf.Print(accuracy, data=[accuracy], message=\"accuracy:\")\n",
    "\n",
    "# We add the training operation, ...\n",
    "adam = tf.train.AdamOptimizer(1e-2)\n",
    "train_op = adam.minimize(loss_op, name=\"train_op\")\n",
    "\n",
    "startTime = time.time()\n",
    "with tf.Session() as sess:\n",
    "    # ... init our variables, ...\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # ... check the accuracy before training, ...\n",
    "    x_input, y_input = sess.run([x_inputs_data, y_inputs_data])\n",
    "    sess.run(accuracy, feed_dict={\n",
    "        input: x_input,\n",
    "        y_true: y_input\n",
    "    })\n",
    "\n",
    "    # ... train ...\n",
    "    for i in range(5000):\n",
    "        #  ... by sampling some input data (fetching) ...\n",
    "        x_input, y_input = sess.run([x_inputs_data, y_inputs_data])\n",
    "        # ... and feeding it to our model\n",
    "        _, loss = sess.run([train_op, loss_op], feed_dict={\n",
    "            input: x_input,\n",
    "            y_true: y_input\n",
    "        })\n",
    "\n",
    "        # We regularly check the loss\n",
    "        if i % 500 == 0:\n",
    "            print('iter:%d - loss:%f' % (i, loss))\n",
    "\n",
    "    # Finally, we check our final accuracy\n",
    "    x_input, y_input = sess.run([x_inputs_data, y_inputs_data])\n",
    "    sess.run(accuracy, feed_dict={\n",
    "        input: x_input,\n",
    "        y_true: y_input\n",
    "    })\n",
    "\n",
    "print(\"Time taken: %f\" % (time.time() - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "위에서 언급한 바대로 '데이터 입력' 부분과 '연산' 부분을 비동기화하여 GPU가 쉬지 않고 일할 수 잇다면, 훨씬 효율적으로 연산 자원을 활용할 수 있을 것이다. 우리는 multi-threading을 사용하여 데이터 입력 부분을 효율적으로 만들어줄 수 있다. TensorFlow에서는 이를 위해 Queue와 Queue runner를 API로 제공하고 있다.\n",
    "\n",
    "reference : https://blog.metaflow.fr/tensorflow-how-to-optimise-your-input-pipeline-with-queues-and-multi-threading-e7c3874157e0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queue_runner 사용 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# This time, let's start with 6 samples of 1 data point\n",
    "x_input_data = tf.random_normal([6], mean=-1, stddev=4)\n",
    "\n",
    "# Note that the FIFO queue has still a capacity of 3\n",
    "q = tf.FIFOQueue(capacity=3, dtypes=tf.float32)\n",
    "\n",
    "# To check what is happening in this case:\n",
    "# we will print a message each time \"x_input_data\" is actually computed\n",
    "# to be used in the \"enqueue_many\" operation\n",
    "x_input_data = tf.Print(x_input_data, data=[x_input_data], message=\"Raw inputs data generated:\", summarize=6)\n",
    "enqueue_op = q.enqueue_many(x_input_data)\n",
    "\n",
    "# To leverage multi-threading we create a \"QueueRunner\"\n",
    "# that will handle the \"enqueue_op\" outside of the main thread\n",
    "# We don't need much parallelism here, so we will use only 1 thread\n",
    "numberOfThreads = 1 \n",
    "qr = tf.train.QueueRunner(q, [enqueue_op] * numberOfThreads)\n",
    "# Don't forget to add your \"QueueRunner\" to the QUEUE_RUNNERS collection\n",
    "tf.train.add_queue_runner(qr) \n",
    "\n",
    "input = q.dequeue() \n",
    "input = tf.Print(input, data=[q.size(), input], message=\"Nb elements left, input:\")\n",
    "\n",
    "# fake graph: START\n",
    "y = input + 1\n",
    "# fake graph: END \n",
    "\n",
    "# We start the session as usual ...\n",
    "with tf.Session() as sess:\n",
    "    # But now we build our coordinator to coordinate our child threads with\n",
    "    # the main thread\n",
    "    coord = tf.train.Coordinator()\n",
    "    # Beware, if you don't start all your queues before runnig anything\n",
    "    # The main threads will wait for them to start and you will hang again\n",
    "    # This helper start all queues in tf.GraphKeys.QUEUE_RUNNERS\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    # The QueueRunner will automatically call the enqueue operation\n",
    "    # asynchronously in its own thread ensuring that the queue is always full\n",
    "    # No more hanging for the main process, no more waiting for the GPU\n",
    "    sess.run(y)\n",
    "    sess.run(y) \n",
    "    sess.run(y)\n",
    "    sess.run(y)\n",
    "    sess.run(y)\n",
    "    sess.run(y)\n",
    "    sess.run(y)\n",
    "    sess.run(y)\n",
    "    sess.run(y)\n",
    "    sess.run(y)\n",
    "\n",
    "    # We request our child threads to stop ...\n",
    "    coord.request_stop()\n",
    "    # ... and we wait for them to do so before releasing the main thread\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개선된 문제점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0 - loss:2.171846\n",
      "iter:500 - loss:0.469907\n",
      "iter:1000 - loss:0.633990\n",
      "iter:1500 - loss:1.030779\n",
      "iter:2000 - loss:0.545669\n",
      "iter:2500 - loss:0.323410\n",
      "iter:3000 - loss:0.787239\n",
      "iter:3500 - loss:0.652673\n",
      "iter:4000 - loss:0.695005\n",
      "iter:4500 - loss:1.328690\n",
      "Time taken: 4.120565\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "# We simulate some raw input data \n",
    "# (think about it as fetching some data from the file system)\n",
    "# let's say: batches of 128 samples, each containing 1024 data points\n",
    "x_input_data = tf.random_normal([128, 1024], mean=0, stddev=1)\n",
    "\n",
    "# We build our small model: a basic two layers neural net with ReLU\n",
    "\n",
    "with tf.variable_scope(\"While_Queue_runner\"):\n",
    "\n",
    "    with tf.variable_scope(\"queue\"):\n",
    "        q = tf.FIFOQueue(capacity=5, dtypes=tf.float32) # enqueue 5 batches\n",
    "        # We use the \"enqueue\" operation so 1 element of the queue is the full batch\n",
    "        enqueue_op = q.enqueue(x_input_data)\n",
    "        numberOfThreads = 1\n",
    "        qr = tf.train.QueueRunner(q, [enqueue_op] * numberOfThreads)\n",
    "        tf.train.add_queue_runner(qr)\n",
    "        input = q.dequeue() # It replaces our input placeholder\n",
    "        # We can also compute y_true right into the graph now\n",
    "        y_true = tf.cast(tf.reduce_sum(input, axis=1, keep_dims=True) > 0, tf.int32)\n",
    "\n",
    "    with tf.variable_scope('FullyConnected'):\n",
    "        w = tf.get_variable('w', shape=[1024, 1024], initializer=tf.random_normal_initializer(stddev=1e-1))\n",
    "        b = tf.get_variable('b', shape=[1024], initializer=tf.constant_initializer(0.1))\n",
    "        z = tf.matmul(input, w) + b\n",
    "        y = tf.nn.relu(z)\n",
    "\n",
    "        w2 = tf.get_variable('w2', shape=[1024, 1], initializer=tf.random_normal_initializer(stddev=1e-1))\n",
    "        b2 = tf.get_variable('b2', shape=[1], initializer=tf.constant_initializer(0.1))\n",
    "        z = tf.matmul(y, w2) + b2\n",
    "\n",
    "    with tf.variable_scope('Loss'):\n",
    "        losses = tf.nn.sigmoid_cross_entropy_with_logits(None, tf.cast(y_true, tf.float32), z)\n",
    "        loss_op = tf.reduce_mean(losses)\n",
    "\n",
    "    with tf.variable_scope('Accuracy'):\n",
    "        y_pred = tf.cast(z > 0, tf.int32)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y_true), tf.float32))\n",
    "        accuracy = tf.Print(accuracy, data=[accuracy], message=\"accuracy:\")\n",
    "\n",
    "    # We add the training op ...\n",
    "    adam = tf.train.AdamOptimizer(1e-2)\n",
    "    train_op = adam.minimize(loss_op, name=\"train_op\")\n",
    "\n",
    "startTime = time.time()\n",
    "with tf.Session() as sess:\n",
    "    # ... init our variables, ...\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # ... add the coordinator, ...\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    # ... check the accuracy before training (without feed_dict!), ...\n",
    "    sess.run(accuracy)\n",
    "\n",
    "    # ... train ...\n",
    "    for i in range(5000):\n",
    "        #  ... without sampling from Python and without a feed_dict !\n",
    "        _, loss = sess.run([train_op, loss_op])\n",
    "\n",
    "        # We regularly check the loss\n",
    "        if i % 500 == 0:\n",
    "            print('iter:%d - loss:%f' % (i, loss))\n",
    "\n",
    "    # Finally, we check our final accuracy\n",
    "    sess.run(accuracy)\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "\n",
    "print(\"Time taken: %f\" % (time.time() - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reference\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "\n",
    "https://www.tensorflow.org/guide/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing data with Tensorflow tf.data API\n",
    "\n",
    "tf.data는 간단한 수준에서 복잡한 수준까지의 Input pipeline을 구성할 수 있도록 하는 API이다. 구체적으로 제공하는 기능은 다음과 같다.\n",
    "\n",
    "# 1. tf.data.Dataset\n",
    "\n",
    "tf.data.Dataset은 연속된 element 집합으로, 각각의 element가 Tensor object로 구성되어있다. 이때 각각의 element는 Training을 위한 data와 label의 pair로 볼 수 있을 것이다.\n",
    "\n",
    "크게 다음과 같은 구성을 가진다.\n",
    "\n",
    "- Creating source : 다수의 tf.Tensor object 또는 file로부터 dataset을 구성한다.\n",
    "  \n",
    "  다양한 source로부터의 입력\n",
    "   1. local file system\n",
    "   2. distributed file system\n",
    "   3. On-memory data\n",
    "   4. real-time data generator\n",
    "  \n",
    "  e.g : Dataset.from_tensor_slices() , on-memory data\n",
    "  \n",
    "  그 외에도 from_generator, list_files, interleave, tfrecordReader\n",
    "\n",
    "\n",
    "- Applying a transformation : 하나 또는 여러 개의 dataset object들로부터 새로운 dataset을 구성한다.\n",
    "\n",
    "  e.g : Dataset.batch()\n",
    "  \n",
    "  그 외에도 concatenate, filter, reduce, map, flat_map, padded_batch, shard, zip, shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of data source\n",
    "1. tf.data.Dataset.from_tensors() or tf.data.Dataset.from_tensor_slices() :\n",
    "   memory 상의 tensor들로 dataset을 만드는 경우\n",
    "2. tf.data.TFRecordDataset :\n",
    "   Disk 상의 file들로 dataset을 만드는 경우\n",
    "3. tf.data.from_generator  :\n",
    "   python iterator로부터 dataset을 만드는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create from_tensor_slice dataset and Show information\n",
    "output_shapes : dataset의 각 element의 shape 정보\n",
    "output_types : dataset의 각 element의 type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset1 : \n",
      "<dtype: 'float32'>\n",
      "(10,)\n",
      "dataset2 : \n",
      "(tf.float32, tf.int32)\n",
      "(TensorShape([Dimension(1)]), TensorShape([Dimension(10)]))\n",
      "dataset3 : \n",
      "(tf.float32, (tf.float32, tf.int32))\n",
      "(TensorShape([Dimension(10)]), (TensorShape([Dimension(1)]), TensorShape([Dimension(10)])))\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices( tf.random_uniform([4, 10], dtype = tf.float32) )\n",
    "print(\"dataset1 : \")\n",
    "print(dataset1.output_types)\n",
    "print(dataset1.output_shapes)\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices( \n",
    "    (tf.random_uniform( [4, 1] ),\n",
    "     tf.random_uniform( [4, 10], maxval=100, dtype = tf.int32)))\n",
    "\n",
    "print(\"dataset2 : \")\n",
    "print(dataset2.output_types)\n",
    "print(dataset2.output_shapes)\n",
    "    \n",
    "dataset3 = tf.data.Dataset.zip( (dataset1, dataset2) )\n",
    "print(\"dataset3 : \")\n",
    "print(dataset3.output_types)\n",
    "print(dataset3.output_shapes)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset and Show information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'/home/dan/datasets/flower_photos/daisy/3506866918_61dd5fc53b_n.jpg'\n",
      "b'/home/dan/datasets/flower_photos/daisy/450128527_fd35742d44.jpg'\n",
      "b'/home/dan/datasets/flower_photos/daisy/6910811638_aa6f17df23.jpg'\n",
      "b'/home/dan/datasets/flower_photos/daisy/2349640101_212c275aa7.jpg'\n",
      "b'/home/dan/datasets/flower_photos/daisy/8120563761_ed5620664f_m.jpg'\n",
      "b'/home/dan/datasets/flower_photos/daisy/14613443462_d4ed356201.jpg'\n",
      "b'/home/dan/datasets/flower_photos/daisy/8887005939_b19e8305ee.jpg'\n",
      "b'/home/dan/datasets/flower_photos/daisy/4511693548_20f9bd2b9c_m.jpg'\n",
      "b'/home/dan/datasets/flower_photos/daisy/3703643767_dee82cdef9_n.jpg'\n",
      "b'/home/dan/datasets/flower_photos/daisy/10437770546_8bb6f7bdd3_m.jpg'\n"
     ]
    }
   ],
   "source": [
    "file_list_dataset = tf.data.Dataset.list_files(\n",
    "    '/home/dan/datasets/flower_photos/daisy/*.jpg',\n",
    "    shuffle=None,\n",
    "    seed=None\n",
    ")\n",
    "iterator = file_list_dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(10):\n",
    "        file = sess.run(next_element)\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 데이터를 표현하는 데이터셋을 만든 뒤에, 데이터셋의 element에 접근하는 iterator를 만들어야 한다. tf.data API는 다음과 같은 iterator를 제공하고 있다. \n",
    "\n",
    "- one-shot,\n",
    "- initializable,\n",
    "- reinitializable, and\n",
    "- feedable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 one_shot_iterator\n",
    "\n",
    "one shot iterator는 가장 간단한 형태의 iterator이다. 이 iterator는 단 한 번의 iterating을 수행하는 것을 지원한다. 어떠한 초기값도 필요하지 않고, 대부분의 queue-base input pipeline에서 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(10):\n",
    "      value = sess.run(next_element)\n",
    "      print(value)\n",
    "      assert i == value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "[11 12 13 14 15 16 17 18 19 20 21]\n",
      "[22 23 24 25 26 27 28 29 30 31 32]\n",
      "[33 34 35 36 37 38 39 40 41 42 43]\n",
      "[44 45 46 47 48 49 50 51 52 53 54]\n",
      "[55 56 57 58 59 60 61 62 63 64 65]\n",
      "[66 67 68 69 70 71 72 73 74 75 76]\n",
      "[77 78 79 80 81 82 83 84 85 86 87]\n",
      "[88 89 90 91 92 93 94 95 96 97 98]\n",
      "[99]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "dataset = dataset.batch(11)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(10):\n",
    "      value = sess.run(next_element)\n",
    "      print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OutOfRangeError when index == 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfRangeError",
     "evalue": "End of sequence\n\t [[node IteratorGetNext_19 (defined at <ipython-input-42-29d07066bdb3>:3)  = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_15)]]\n\nCaused by op 'IteratorGetNext_19', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-42-29d07066bdb3>\", line 3, in <module>\n    next_element = iterator.get_next()\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 421, in get_next\n    name=name)), self._output_types,\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2069, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nOutOfRangeError (see above for traceback): End of sequence\n\t [[node IteratorGetNext_19 (defined at <ipython-input-42-29d07066bdb3>:3)  = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_15)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence\n\t [[{{node IteratorGetNext_19}} = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_15)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-29d07066bdb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0;32massert\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence\n\t [[node IteratorGetNext_19 (defined at <ipython-input-42-29d07066bdb3>:3)  = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_15)]]\n\nCaused by op 'IteratorGetNext_19', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-42-29d07066bdb3>\", line 3, in <module>\n    next_element = iterator.get_next()\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 421, in get_next\n    name=name)), self._output_types,\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2069, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/home/dan/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nOutOfRangeError (see above for traceback): End of sequence\n\t [[node IteratorGetNext_19 (defined at <ipython-input-42-29d07066bdb3>:3)  = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_15)]]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(101):\n",
    "      value = sess.run(next_element)\n",
    "      assert i == value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 initializable iterator\n",
    "\n",
    "경우에 따라 항상 일정한 dataset을 사용하는 것이 아니라 적절한 parameter를 주어서 dataset을 제어해야하는 경우가 있다.\n",
    "이 경우 initializable iterator는 session을 통해 한 번 초기화가 필요한 대신 dataset의 정의를 제어할 수 있다.\n",
    "\n",
    "다음의 예시는 tf.placeholder를 통해 dataset의 parameter를 initializable iterator에 fed하는 방식을 소개한다.\n",
    "위에서와 마찬가지로 Dataset.range 함수를 사용하나 max_value를 parameter로 제어하는 예시이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = tf.placeholder(tf.int64, shape=[])\n",
    "dataset = tf.data.Dataset.range(max_value)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize an iterator over a dataset with 10 elements.\n",
    "    sess.run(iterator.initializer, feed_dict={max_value: 10})\n",
    "    for i in range(10):\n",
    "      value = sess.run(next_element)\n",
    "      assert i == value\n",
    "\n",
    "    # Initialize the same iterator over a dataset with 100 elements.\n",
    "    sess.run(iterator.initializer, feed_dict={max_value: 100})\n",
    "    for i in range(100):\n",
    "      value = sess.run(next_element)\n",
    "      assert i == value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 reinitializable iterator\n",
    "\n",
    "reinitializable iterator는 두 개 이상의 dataset을 다른 방식으로 제어하고 싶을 때 사용한다.\n",
    "예를 들면 아래의 예시는 training dataset과 validation dataset 두 가지를 사용할 때, 우리는 경우에 따라서 training dataset에 generalization을 위해 perturbation을 추가하는 경우가 많다. 반면에 validation dataset에는 그러한 처리가 들어가지 않기를 바랄 수 있다. 그런 경우 reinitializable iterator를 사용하면 편리하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training and validation datasets with the same structure.\n",
    "training_dataset = tf.data.Dataset.range(100).map(\n",
    "    lambda x: x + tf.random_uniform([], -10, 10, tf.int64))\n",
    "validation_dataset = tf.data.Dataset.range(50)\n",
    "\n",
    "# A reinitializable iterator is defined by its structure. We could use the\n",
    "# `output_types` and `output_shapes` properties of either `training_dataset`\n",
    "# or `validation_dataset` here, because they are compatible.\n",
    "iterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n",
    "                                           training_dataset.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n",
    "validation_init_op = iterator.make_initializer(validation_dataset)\n",
    "\n",
    "# Run 20 epochs in which the training dataset is traversed, followed by the\n",
    "# validation dataset.\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for _ in range(20):\n",
    "      # Initialize an iterator over the training dataset.\n",
    "      sess.run(training_init_op)\n",
    "      for _ in range(100):\n",
    "        sess.run(next_element)\n",
    "\n",
    "      # Initialize an iterator over the validation dataset.\n",
    "      sess.run(validation_init_op)\n",
    "      for _ in range(50):\n",
    "        sess.run(next_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 feedable iterator\n",
    "\n",
    "feedable iterator는 기본적으로 reinitializable iterator와 동일한 mechanism을 갖는다. 그러나 좀 더 iterator를 유연하게 스위칭할 수 있게 해준다. reinitialzable iterator는 iterator 스위칭을 위해 매번 초기화를 다시 해줘야 하지만 feedable iterator는 우리에게 익숙한 feed_dict와 placeholder를 사용하여 좀 더 유연하게 iterator를 변경하며 사용할 수 있게 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training and validation datasets with the same structure.\n",
    "training_dataset = tf.data.Dataset.range(100).map(\n",
    "    lambda x: x + tf.random_uniform([], -10, 10, tf.int64)).repeat()\n",
    "validation_dataset = tf.data.Dataset.range(50)\n",
    "\n",
    "# A feedable iterator is defined by a handle placeholder and its structure. We\n",
    "# could use the `output_types` and `output_shapes` properties of either\n",
    "# `training_dataset` or `validation_dataset` here, because they have\n",
    "# identical structure.\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(\n",
    "    handle, training_dataset.output_types, training_dataset.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# You can use feedable iterators with a variety of different kinds of iterator\n",
    "# (such as one-shot and initializable iterators).\n",
    "training_iterator = training_dataset.make_one_shot_iterator()\n",
    "validation_iterator = validation_dataset.make_initializable_iterator()\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# The `Iterator.string_handle()` method returns a tensor that can be evaluated\n",
    "# and used to feed the `handle` placeholder.\n",
    "training_handle = sess.run(training_iterator.string_handle())\n",
    "validation_handle = sess.run(validation_iterator.string_handle())\n",
    "\n",
    "# Loop forever, alternating between training and validation.\n",
    "for i in range(10):\n",
    "  # Run 200 steps using the training dataset. Note that the training dataset is\n",
    "  # infinite(training_dataset has repeat() method at the end), \n",
    "  # and we resume from where we left off in the previous for loop iteration.\n",
    "  for _ in range(200):\n",
    "    sess.run(next_element, feed_dict={handle: training_handle})\n",
    "\n",
    "  # Run one pass over the validation dataset.\n",
    "  sess.run(validation_iterator.initializer)\n",
    "  for _ in range(50):\n",
    "    sess.run(next_element, feed_dict={handle: validation_handle})\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
